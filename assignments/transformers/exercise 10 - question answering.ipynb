{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 - Question Answering in NLP\n",
    "In this exercise, you will experiment with one of NLPâ€™s exciting tasks - Question Answering!\n",
    "\n",
    "You will first evaluate a pre-trained model on Squad, a leading question-answering dataset, and evaluate its performance. Those with an approved access to GPUs in AWS or a different provider are encouraged to also fine-tune a base model on the Squad dataset.\n",
    "\n",
    "We will use HuggingFaceâ€™s Transformers, the leading package for NLP tasks using transformers. Your code should roughly follow the code of [this guide](https://huggingface.co/docs/transformers/tasks/question_answering) and [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).  \n",
    "\n",
    "(**Important Note:** The guide writes considerable amount of code to handle the case of context longer than the max input sequence. For simplicity, in your code, you should remove from the datasets all contexts longer than \n",
    "`max_length = 384`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Tracking using ClearML\n",
    "This exercise utilizes large models. While we only fine-tune existing models, the time required for fine-tuning is still large (over one hour of run on a V100 GPU in many cases). You are not expected to make many runs yourself, but we would like to benefit from the joint work of the entire class, and will therefore report our runs to ClearML.\n",
    "\n",
    "We will use a shared ClearML project for all student groups, so you can see runs of other teams (see instructions below). Specifically, we will choose a set of hyper-parameters for your own runs, and compare them to others, so we benefit from the community runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the transformers, datasets libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-14T06:32:53.448945Z",
     "start_time": "2022-05-14T06:32:08.776112Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries.  \n",
    "Make sure your version of Transformers is at least 4.11.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T19:21:15.737679Z",
     "start_time": "2022-05-18T19:21:15.663862Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the ðŸ¤— [Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions load_dataset and load_metric.\n",
    "\n",
    "For our example here, we'll use version 1.1 of Stanford's [SQUAD dataset](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Squad v1.1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T19:21:57.385828Z",
     "start_time": "2022-05-18T19:21:54.920317Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets object itself is DatasetDict, which contains one key for the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-19T05:54:43.776922Z",
     "start_time": "2022-05-19T05:54:43.770922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the training, validation and test sets all have a column for the context, the question and the answers to those questions.\n",
    "\n",
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T19:22:38.535553Z",
     "start_time": "2022-05-18T19:22:38.442614Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, answer these questions:  \n",
    "\n",
    "What is the shortest context in the training dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T19:22:57.607768Z",
     "start_time": "2022-05-18T19:22:46.559700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the longest answer in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T19:26:42.399583Z",
     "start_time": "2022-05-18T19:26:34.184166Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a question that appears multiple times? What is the most common question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T19:33:32.003715Z",
     "start_time": "2022-05-18T19:33:23.799826Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know the HuggingFace transformersâ€™ tokenizers\n",
    "As a preprocessing step, the HuggingFace code tokenizes input sequences using a Tokenizer. Read more about tokenizers here:\n",
    "https://huggingface.co/docs/tokenizers/pipeline  \n",
    "https://huggingface.co/transformers/v3.0.2/preprocessing.html\n",
    "\n",
    "For this question, use the BERT tokenizer. The tokenizer sometimes breaks words into smaller chunks, so the number of tokens can be larger than the number of words. \n",
    "\n",
    "Using the first 1,000 context datapoints, print the 30 most common tokens by the tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T19:36:30.771751Z",
     "start_time": "2022-05-18T19:36:30.758022Z"
    }
   },
   "source": [
    "### Load a pretrained Question Answering model\n",
    "In this section, you will use a model pretrained on the Squad dataset for question answering.  \n",
    "\n",
    "Choose a model you'd like to use.  \n",
    "You can see a list of available models here: https://huggingface.co/models?dataset=dataset:squad&sort=downloads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Model Error Analysis\n",
    "Here you will evaluate your modelâ€™s performance.\n",
    "\n",
    "Write code to manually review a few errors of the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see a pattern there? Is there any hypothesis you form for cases where the model fails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that runs inference and outputs the predicted answer to a context and question texts typed by the user. We recommend that you use ipywidgets for interactivity:  \n",
    "https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Basics.html\n",
    "\n",
    "Use the award-winning GUI youâ€™ve just created, to try to manually poke holes in the model. Try to characterize the cases your model mishandles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, evaluate your modelâ€™s performance for different lengths of input text and of answer length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BONUS: Can you think of other axes that would be interesting to use to evaluate your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Advanced] Fine-tune a Model\n",
    "Here, you will fine-tune a base model to the Squad dataset, and evaluate its performance. \n",
    "\n",
    "Note that since the model is large, running this exercise might cost dozens of dollars. Youâ€™re very encouraged to use your AWS credits for that. We also recommend to work on the code on a cheap CPU-based machine, make sure that it runs properly, and then move to more expensive instances with GPUs for actual runs.\n",
    "\n",
    "What metric do you find suited? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model to fine-tune on the dataset. \n",
    "\n",
    "Make sure you use ClearML to report your results in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-14T08:24:02.833322Z",
     "start_time": "2022-05-14T08:23:58.957315Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write below your train and validation loss. You can compare your results to those of others from the class using ClearML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClearML Integration\n",
    "You can join the ClearML slack channel at  https://join.slack.com/t/clearml/shared_invite/zt-c0t13pty-aVUZZW1TSSSg2vyIGVPBhg to ask questions and see real life examples and questions of industry users.\n",
    "\n",
    "### Parameters & Configurations\n",
    "Keep your parameters/configs in a single dict within your code. For example \n",
    "\n",
    "`config={\"param\":\"data\", ...}`\n",
    "\n",
    "This way you can easily connect into clearml using\n",
    "\n",
    "`Task.connect(config)`\n",
    "\n",
    "The documentation is here:\n",
    "https://clear.ml/docs/latest/docs/references/sdk/task#connect_configuration\n",
    "\n",
    "\n",
    "### Comparisons\n",
    "Once you gathered some data, you can select multiple experiments and compare them, as detailed here: https://clear.ml/docs/latest/docs/webapp/webapp_exp_comparing/\n",
    "\n",
    "### Additional Videos\n",
    "We also recommend you review these videos to learn industry best practices:\n",
    "- Day in the life of a data scientist - This video will cover nearly everything your might need to use clearml - https://www.youtube.com/watch?v=quSGXvuK1IM\n",
    "- Detection in video on raspberry pi â€“ a real world example of what can be done with ML (an example of a nice portfolio project) -  https://www.youtube.com/watch?v=ZiOr9EdYEeE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Resources\n",
    "For an open discussion on Question Answering related topics, you are very encouraged to watch this workshop: https://www.youtube.com/watch?v=Ihgk8kGLpIE\n",
    "\n",
    "This screencast uses T5 on a different Q&A dataset: https://www.youtube.com/watch?v=_l2wJb3QPdk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it - good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
