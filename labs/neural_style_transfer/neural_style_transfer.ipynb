{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdDVUaMJHylw"
   },
   "source": [
    "# Neural Style Transfer\n",
    "https://towardsdatascience.com/implementing-neural-style-transfer-using-pytorch-fd8d43fb7bfa\n",
    "\n",
    "https://github.com/Octaves0911/Neural_Style_Transfer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:32:30.445381Z",
     "start_time": "2021-03-16T03:32:26.363098Z"
    },
    "id": "MK_bz7tRHymM"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Nov  5 02:21:36 2020\n",
    "@author: Aman Kumar Mallik\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#importing the required libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:34:36.964530Z",
     "start_time": "2021-03-16T03:32:34.295790Z"
    },
    "id": "cAvQE-UZHymZ"
   },
   "outputs": [],
   "source": [
    "#Loadung the model vgg19 that will serve as the base model\n",
    "model=models.vgg19(pretrained=True).features\n",
    "# the vgg19 model has three components :\n",
    "    #features: containg all the conv, relu and maxpool\n",
    "    #avgpool: containing the avgpool layer\n",
    "    #classifier: contains the Dense layer(FC part of the model) \n",
    "\n",
    "\n",
    "#Assigning the GPU to the variable device\n",
    "device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:34:36.994725Z",
     "start_time": "2021-03-16T03:34:36.983615Z"
    },
    "id": "EUWdqFVFHyme"
   },
   "outputs": [],
   "source": [
    "#[0,5,10,19,28] are the index of the layers we will be using to calculate the loss as per the paper of NST\n",
    "#Defining a class that for the model\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG,self).__init__()\n",
    "        #Here we will use the following layers and make an array of their indices\n",
    "        # 0: block1_conv1\n",
    "        # 5: block2_conv1\n",
    "        # 10: block3_conv1\n",
    "        # 19: block4_conv1\n",
    "        # 28: block5_conv1\n",
    "        self.req_features= ['0','5','10','19','28'] \n",
    "        #Since we need only the 5 layers in the model so we will be dropping all the rest layers from the features of the model\n",
    "        self.model=models.vgg19(pretrained=True).features[:29] #model will contain the first 29 layers\n",
    "    \n",
    "   \n",
    "    #x holds the input tensor(image) that will be feeded to each layer\n",
    "    def forward(self,x):\n",
    "        #initialize an array that wil hold the activations from the chosen layers\n",
    "        features=[]\n",
    "        #Iterate over all the layers of the mode\n",
    "        for layer_num,layer in enumerate(self.model):\n",
    "            #activation of the layer will stored in x\n",
    "            x=layer(x)\n",
    "            #appending the activation of the selected layers and return the feature array\n",
    "            if (str(layer_num) in self.req_features):\n",
    "                features.append(x)\n",
    "                \n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defing a function that will load the image and perform the required preprocessing and put it on the GPU\n",
    "def image_loader(path):\n",
    "    image=Image.open(path)\n",
    "    #defining the image transformation steps to be performed before feeding them to the model\n",
    "    loader=transforms.Compose([transforms.Resize((512,512)),transforms.ToTensor()])\n",
    "    #The preprocessing steps involves resizing the image and then converting it to a tensor\n",
    "\n",
    "    image=loader(image).unsqueeze(0)\n",
    "    return image.to(device,torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fXqi0AplHymh"
   },
   "outputs": [],
   "source": [
    "#Loading the original and the style image\n",
    "# original_image=image_loader('dancing.jpg')\n",
    "original_image=image_loader('dancing_200x200.jpg')\n",
    "\n",
    "# style_image=image_loader('picasso.jpg')\n",
    "# style_image=image_loader('kusama.jpg')\n",
    "style_image=image_loader('ThatsSoTampa-Kasuma-TMA-web-18.jpg')\n",
    "# style_image=image_loader('digital-remastered-edition-blue-painting-wassily-kandinsky.jpg')\n",
    "# style_image=image_loader('picasso_100x100.jpg')\n",
    "\n",
    "#Creating the generated image from the original image\n",
    "generated_image=original_image.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5n1XO7Q9Hymk"
   },
   "outputs": [],
   "source": [
    "#Load the model to the GPU\n",
    "with torch.no_grad():\n"
    "    model=VGG().to(device).eval() \n",
    "    orig_feautes = model(original_image)\n",
    "    style_featues = model(style_image)\n",
    "\n",
    "#initialize the paramerters required for fitting the model\n",
    "epoch = 300\n",
    "lr = 0.05\n",
    "alpha = 1\n",
    "beta = 140\n",
    "\n",
    "#using adam optimizer and it will update the generated image not the model parameter \n",
    "optimizer=optim.Adam([generated_image], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:34:37.026742Z",
     "start_time": "2021-03-16T03:34:37.013701Z"
    },
    "id": "FeMc7B2RHymj"
   },
   "outputs": [],
   "source": [
    "def calc_content_loss(gen_feat,orig_feat):\n",
    "    #calculating the content loss of each layer by calculating the MSE between the content and generated features and adding it to content loss\n",
    "    content_l=torch.mean((gen_feat-orig_feat)**2)#*0.5\n",
    "    return content_l\n",
    "\n",
    "def calc_style_loss(gen,style):\n",
    "    #Calculating the gram matrix for the style and the generated image\n",
    "    batch_size,channel,height,width=gen.shape\n",
    "\n",
    "    G=torch.mm(gen.view(channel,height*width),gen.view(channel,height*width).t())\n",
    "    A=torch.mm(style.view(channel,height*width),style.view(channel,height*width).t())\n",
    "        \n",
    "    #Calcultating the style loss of each layer by calculating the MSE between the gram matrix of the style image and the generated image and adding it to style loss\n",
    "    style_l=torch.mean((G-A)**2)#/(4*channel*(height*width)**2)\n",
    "    return style_l\n",
    "\n",
    "def calculate_loss(gen_features, orig_feautes, style_featues):\n",
    "    style_loss=content_loss=0\n",
    "    for gen,cont,style in zip(gen_features,orig_feautes,style_featues):\n",
    "        #extracting the dimensions from the generated image\n",
    "        content_loss += calc_content_loss(gen,cont)\n",
    "        style_loss += calc_style_loss(gen,style)\n",
    "    \n",
    "    #calculating the total loss of e th epoch\n",
    "    total_loss = alpha*content_loss + beta*style_loss \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c58w53DMHymm",
    "outputId": "fe8deb2f-a5f7-493a-d5de-7952f43da7a0"
   },
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "\n",
    "#iterating for 1000 times\n",
    "for e in range (epoch):\n",
    "    #extracting the features of generated, content and the original required for calculating the loss\n",
    "    gen_features = model(generated_image)\n",
    "    \n",
    "    #iterating over the activation of each layer and calculate the loss and add it to the content and the style loss\n",
    "    total_loss = calculate_loss(gen_features, orig_feautes, style_featues)\n",
    "    #optimize the pixel values of the generated image and backpropagate the loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print the image and save it after each 100 epoch\n",
    "    if(not (e%20)):\n",
    "        print(total_loss)\n",
    "        \n",
    "        save_image(generated_image,\"generated.png\")  \n",
    "        display.display(display.Image(filename='generated.png')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwcspnyeJwXo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "neural_style_transfer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
